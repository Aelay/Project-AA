{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "from statistics import StatisticsError\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import itertools\n",
    "from sklearn import mixture\n",
    "\n",
    "\n",
    "def L1 (x,y):\n",
    "    dist = 0\n",
    "    if len(x)==len(y):\n",
    "        for i in range(len(x)):\n",
    "            dist += math.fabs(x[i]-y[i])\n",
    "        return(dist)\n",
    "    else:\n",
    "        print('vectors must be equal length for L1')\n",
    "        return (None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!\n",
    "# This code here makes the nx.Graph\n",
    "G=nx.Graph()\n",
    "\n",
    "m=0 # these two counters \n",
    "n=0 # arn't important\n",
    "\n",
    "with open('training.1600000.processed.noemoticon.csv', encoding='latin-1') as f_in:\n",
    "    for line in f_in: \n",
    "        lineX = list(csv.reader(line, skipinitialspace=True))\n",
    "        G.add_node(lineX[8][0])\n",
    "        if '@' in lineX[10][0]:\n",
    "            m+=1\n",
    "            for t in re.split('[^a-zA-Z\\_\\@]', lineX[10][0]):\n",
    "                if t!='' and t[0]=='@' and t!='@':\n",
    "                    G.add_edge(lineX[8][0],t[1:])\n",
    "                    n+=1\n",
    "        if n%100000==0:\n",
    "            print(n)\n",
    "print(nx.number_of_nodes(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finding the largest connected_component\n",
    "LargestCC = max(nx.connected_component_subgraphs(G), key=len) # largest connected component\n",
    "print(nx.number_of_nodes(LargestCC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "core7 = nx.k_core(LargestCC,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the fiedler vector, and use it to partition the graph\n",
    "\n",
    "f = nx.fiedler_vector(core7)\n",
    "s = np.zeros(len(f),dtype='int')\n",
    "s[f>0]=1\n",
    "\n",
    "# this is the positions we will use for each graph\n",
    "pos = nx.spring_layout(core7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# draw partition\n",
    "colors = ['#d7191c', '#2b83ba'] # red and blue\n",
    "node_colors = [colors[s[v]] for v in range(nx.number_of_nodes(core7))]\n",
    "nx.draw(core7,pos=pos, node_color=node_colors,node_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this makes the laplacian matrix to do the spectral clustering\n",
    "L = nx.laplacian_matrix(core7).todense()\n",
    "w, v = np.linalg.eig(L)\n",
    "v = np.array(v)\n",
    "worder = np.argsort(w)\n",
    "\n",
    "X = v @ np.diag(w)\n",
    "X = X[:,worder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# based on the graph above, k=6 was chosen. k=4 was what we were taught to\n",
    "# choose, because it's the \"L\" in the graph. Though that didn't look good,\n",
    "# so I increased k to 6.\n",
    "# this runs k-means for the next code\n",
    "kmeans = KMeans(init='k-means++', n_clusters=6, n_init=10)\n",
    "kmeans.fit_predict(X[:,1:3])\n",
    "centroids = kmeans.cluster_centers_\n",
    "labels = kmeans.labels_\n",
    "error = kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colors = ['#d7191c', '#ffffbf', '#2b83ba', 'green','orange','maroon']\n",
    "node_colors = [colors[labels[i]] for i in range(nx.number_of_nodes(core7))]\n",
    "nx.draw(core7, pos = pos, node_color=node_colors,node_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we switch from the graphical analysis to LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!\n",
    "# this reads in the tweets\n",
    "# then simply parses user ID into ID_list\n",
    "# and the tweet text into TextList\n",
    "TextList = []\n",
    "ID_list = []\n",
    "n=0\n",
    "with open('training.1600000.processed.noemoticon.csv', encoding='latin-1') as f_in:\n",
    "    for line in f_in:\n",
    "        lineX = list(csv.reader(line, skipinitialspace=True))\n",
    "        TextList.append(lineX[10][0])\n",
    "        ID_list.append(lineX[8][0])\n",
    "        n=n+1\n",
    "        if n%100000==0:\n",
    "            print(n)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!\n",
    "# vectorize TextList to dtm\n",
    "vectorizer = TfidfVectorizer(stop_words='english', min_df=4,max_df=0.8)\n",
    "dtm = vectorizer.fit_transform(TextList)\n",
    "del TextList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!\n",
    "# compute svd of dtm\n",
    "svd = TruncatedSVD(n_components=100, n_iter=4)\n",
    "svdOutput = svd.fit_transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!\n",
    "# this is the model I went with for LSA\n",
    "\n",
    "gmm = mixture.GaussianMixture(n_components=5, covariance_type='full')\n",
    "gmm.fit(svdOutput[:,:15])\n",
    "pred = gmm.predict(svdOutput[:,:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this converts the GMM result from classifying tweets\n",
    "# into classifying users\n",
    "ID_Pred = {}\n",
    "for i in range(len(ID_list)):\n",
    "    ID = ID_list[i]\n",
    "    if ID in ID_Pred:\n",
    "        ID_Pred[ID].append(pred[i])\n",
    "    else:\n",
    "        ID_Pred[ID]=[pred[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this converts the GMM result from classifying tweets\n",
    "# into classifying users\n",
    "# this also classifies all users, not just core7\n",
    "ID_Pred = {}\n",
    "for i in range(len(ID_list)):\n",
    "    ID = ID_list[i]\n",
    "    if ID in ID_Pred:\n",
    "        ID_Pred[ID].append(pred[i])\n",
    "    else:\n",
    "        ID_Pred[ID]=[pred[i]]\n",
    "        \n",
    "colors = ['#d7191c', '#ffffbf', '#2b83ba', 'green','orange','maroon','black']\n",
    "node_colors = []\n",
    "for g in core7: # classify the nodes, based off their tweets\n",
    "    try:\n",
    "        try: # if there is only one mode of groups, classify the user as the mode\n",
    "            X = statistics.mode(ID_Pred[g])\n",
    "            node_colors.append(colors[X])\n",
    "        except StatisticsError: # if there is no mode, pick a tweet at random, and classify the user as that tweet's group\n",
    "            node_colors.append(colors[ID_Pred[g][random.randint(0,len(ID_Pred[g])-1)]])\n",
    "            \n",
    "    except KeyError: # if the node never tweeted (was only tweeted at)\n",
    "        node_colors.append(colors[6]) # make it black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# draw the core7 based on LSA predictions, only to make it easier to\n",
    "# compare to our spectral clustering\n",
    "nx.draw(core7,pos=pos, node_color=node_colors,node_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!\n",
    "# this gives the top terms of each eigenvector for our LSA\n",
    "# the groups aren't exactly these values, but it's similar.\n",
    "# you can also plot the nodes , with these eigenvectors as the axis\n",
    "# being a good way to visualize the results of LSA\n",
    "for i in range(0,10):\n",
    "    top = np.argsort(svd.components_[i])\n",
    "    topterms = [terms[top[f]] for f in range(30)]\n",
    "    print()\n",
    "    print (i, topterms)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
